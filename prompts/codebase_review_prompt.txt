########################################################################################
# Codebase Review Prompt â€” **ethereum_project**
########################################################################################

Objective: Please conduct a thorough review of the current project codebase. Your primary goals are to identify areas for improvement in terms of best practices, efficiency, clarity, and robustness.

Referential Documents for Standards and Context:

README.md: For project overview, module descriptions, and intended architecture.

PROJECT_CONFIG_DETAILS.md: For detailed settings of our linters (Ruff, Flake8), formatter (Ruff format), type checker (MyPy), and other development tools. Adherence to these configurations is key.

prompts/starter_prompt.txt: For general collaboration guidelines, code quality expectations (e.g., PEP 8, docstrings, type hints), and design principles (KISS, DRY, YAGNI).

Areas of Focus for Review:

1.  **Adherence to Best Practices & Project Standards**:
    *   Does the code align with Python best practices (PEP 8, idiomatic Python)?
    *   Are project-specific conventions (if any, implied by existing code or `README.md`) followed?
    *   Are docstrings (Google style) and type hints used effectively and consistently?
    *   Is the code aligned with the configurations set in our linting/formatting tools (see `PROJECT_CONFIG_DETAILS.md`)?
    *   Does the code adhere to design principles like KISS, DRY, YAGNI as outlined in `prompts/starter_prompt.txt`?

2.  **Correctness, Robustness & Error Handling**:
    *   Are there any potential bugs, logical errors, or edge cases not handled?
    *   Is error handling robust and user-friendly (e.g., clear logging, appropriate exceptions)?
    *   Are there any race conditions or other concurrency issues (if applicable)?

3.  **Efficiency & Performance**:
    *   Are there any obvious performance bottlenecks (e.g., inefficient loops, redundant computations, suboptimal use of `pandas`/`numpy`)?
    *   Can any sections be optimized for speed or memory usage without sacrificing clarity?

4.  **Clarity, Readability & Maintainability**:
    *   Is the code easy to understand? Are variable and function names clear and descriptive?
    *   Is there unnecessary complexity? Can any parts be simplified for better readability? (KISS)
    *   Does the code behave in an intuitive and predictable manner? (Principle of Least Astonishment)
    *   Are modules and functions well-organized and cohesive?
    *   Is there sufficient commenting for non-obvious logic, while avoiding over-commenting obvious code?

5.  **Modularity & Reusability**:
    *   Are there opportunities to refactor code into more reusable functions or classes? (DRY)
    *   Is there duplicated code that could be centralized? (DRY)
    *   Do modules and functions have a single, well-defined responsibility? (Single Responsibility Principle)

6.  **Testability & Test Coverage (Conceptual)**:
    *   While you may not run the tests, does the code seem structured in a way that is easy to unit test?
    *   Are there any parts that would be particularly difficult to test? (Suggesting refactoring if so).
    *   Are new features and modifications adequately covered by unit tests?

7.  **Consistency**:
    *   Beyond automated checks, is there consistency in coding style, naming conventions, and architectural patterns across different modules?

8.  **Documentation Sync**:
    *   Is `README.md` up-to-date with the current features and project state?
    *   Does `PROJECT_CONFIG_DETAILS.md` accurately reflect all current configurations, especially if new tools/dependencies were added or settings changed?

Quality Scoring Flyâ€‘Wheel ðŸ”„

Before issuing detailed feedback, compute a 0â€“100 score for each of the twelve quality axes listed below and produce the Scoreboard Table plus JSON snapshot.Write artefacts to:

prompts/quality_scoreboard.md â€“ append a new markdown row each run, preserving history.

quality_scoreboard.json â€“ overwrite with the latest snapshot.

Evaluation Axes

1.  **Clarity & Readability** (Simplicity, intuitiveness, naming, comments)
2.  **Documentation Quality** (`README.md`, `PROJECT_CONFIG_DETAILS.md`, inline docs, API refs, docstrings)
3.  **Codingâ€‘Style Consistency** (PEP 8, formatting, idioms, project conventions)
4.  **Complexity Management** (Avoids unnecessary complexity, KISS principle)
5.  **Modularity & Cohesion** (SRP, DRY, reusability, separation of concerns)
6.  **Test Coverage & Quality** (Presence, relevance, and thoroughness of unit tests)
7.  **Performance & Efficiency** (No obvious bottlenecks, optimal data structures/algorithms)
8.  **Error Handling & Resilience** (Graceful failure, clear logging, robust exceptions)
9.  **Dependency & Security Hygiene** (CVEs, deprecated libs, SBOM, basic secure coding practices like input validation if applicable, no hardcoded secrets in code)
10. **Scalability & Extensibility** (Architecture flexibility, OCP considerations)
11. **Versionâ€‘Control Practices** (Commit messages, branching, tagging - evaluated conceptually)
12. **Overall Maintainability** (Ease of onboarding & long-term upkeep)

Scoreboard Table (markdown)

| #  | Axis                    | Score (0â€‘100) | Rationale (â‰¤â€¯1 line) |
| -- | ----------------------- | ------------: | -------------------- |
| 1  | Clarity & Readability   |               |                      |
| â€¦  | â€¦                       |             â€¦ | â€¦                    |
| 12 | Overall Maintainability |               |                      |

After the table, include Strengths, Weaknesses, Detailed Recommendations, Topâ€‘Priority Fixes, and Overall Quality Grade (mean score).If every axis scores 100, output the exact line:
QUALITY PERFECT â€“ all axes 100/100. Halting further tickets. and terminate the cycle.

Requested Output Format:

Please provide your feedback in a structured format. For each identified issue or suggestion, please include:

File Path & Line Number(s): (If applicable)

Issue/Suggestion Description: Clearly explain the point.

Reasoning/Impact: Why is this an issue or why would the suggestion be an improvement (e.g., impacts readability, performance, maintainability)?

Recommended Action: Concrete steps to address it.

Priority/Severity: (Optional, e.g., High/Medium/Low, or Critical/Major/Minor)

Code Citation Format: When pointing to code, cite using Cursor's format startLine:endLine:filepath (e.g., 42:60:src/ols_models.py) so we can jump directly to the snippet.

Example:

*   **File Path & Line Number(s)**: `src/data_processing.py:115-120`
*   **Issue/Suggestion Description**: Redundant data loading.
*   **Reasoning/Impact**: The file `some_data.csv` is loaded twice in the same function, impacting performance for large files.
*   **Recommended Action**: Load the data once into a DataFrame and reuse the DataFrame.
*   **Priority/Severity**: Medium

Please focus on actionable and constructive feedback. Thank you!