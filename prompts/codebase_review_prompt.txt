########################################################################################
# Codebase Review Prompt â€” **ethereum_project**
########################################################################################

Objective: Please conduct a thorough review of the current project codebase. Your primary goals are to identify areas for improvement in terms of best practices, efficiency, clarity, and robustness.

Referential Documents for Standards and Context:

README.md: For project overview, module descriptions, and intended architecture.

PROJECT_CONFIG_DETAILS.md: For detailed settings of our linters (Ruff, Flake8), formatter (Ruff format), type checker (MyPy), and other development tools. Adherence to these configurations is key.

prompts/starter_prompt.txt: For general collaboration guidelines and code quality expectations (e.g., PEP 8, docstrings, type hints).

Areas of Focus for Review:

Adherence to Best Practices & Project Standards:

Does the code align with Python best practices (PEP 8, idiomatic Python)?

Are project-specific conventions (if any, implied by existing code or README.md) followed?

Are docstrings (Google style) and type hints used effectively and consistently?

Is the code aligned with the configurations set in our linting/formatting tools (see PROJECT_CONFIG_DETAILS.md)?

Correctness, Robustness & Error Handling:

Are there any potential bugs, logical errors, or edge cases not handled?

Is error handling robust and user-friendly (e.g., clear logging, appropriate exceptions)?

Are there any race conditions or other concurrency issues (if applicable)?

Efficiency & Performance:

Are there any obvious performance bottlenecks (e.g., inefficient loops, redundant computations, suboptimal use of pandas/numpy)?

Can any sections be optimized for speed or memory usage without sacrificing clarity?

Clarity, Readability & Maintainability:

Is the code easy to understand? Are variable and function names clear and descriptive?

Is there unnecessary complexity? Can any parts be simplified for better readability?

Are modules and functions well-organized and cohesive?

Is there sufficient commenting for non-obvious logic, while avoiding over-commenting obvious code?

Modularity & Reusability:

Are there opportunities to refactor code into more reusable functions or classes?

Is there duplicated code that could be centralized?

Testability & Test Coverage (Conceptual):

While you may not run the tests, does the code seem structured in a way that is easy to unit test?

Are there any parts that would be particularly difficult to test? (Suggesting refactoring if so).

Consistency:

Beyond automated checks, is there consistency in coding style, naming conventions, and architectural patterns across different modules?

Quality Scoring Flyâ€‘Wheel ðŸ”„

Before issuing detailed feedback, compute a 0â€“100 score for each of the twelve quality axes listed below and produce the Scoreboard Table plus JSON snapshot.Write artefacts to:

prompts/quality_scoreboard.md â€“ append a new markdown row each run, preserving history.

quality_scoreboard.json â€“ overwrite with the latest snapshot.

Evaluation Axes

Clarity & Readability

Documentation Quality

Codingâ€‘Style Consistency

Complexity Management

Modularity & Cohesion

Test Coverage & Quality

Performance & Efficiency

Error Handling & Resilience

Dependency & Security Hygiene

Scalability & Extensibility

Versionâ€‘Control Practices

Overall Maintainability

Scoreboard Table (markdown)

| #  | Axis                    | Score (0â€‘100) | Rationale (â‰¤â€¯1 line) |
| -- | ----------------------- | ------------: | -------------------- |
| 1  | Clarity & Readability   |               |                      |
| â€¦  | â€¦                       |             â€¦ | â€¦                    |
| 12 | Overall Maintainability |               |                      |

After the table, include Strengths, Weaknesses, Detailed Recommendations, Topâ€‘Priority Fixes, and Overall Quality Grade (mean score).If every axis scores 100, output the exact line:
QUALITY PERFECT â€“ all axes 100/100. Halting further tickets. and terminate the cycle.

Requested Output Format:

Please provide your feedback in a structured format. For each identified issue or suggestion, please include:

File Path & Line Number(s): (If applicable)

Issue/Suggestion Description: Clearly explain the point.

Reasoning/Impact: Why is this an issue or why would the suggestion be an improvement (e.g., impacts readability, performance, maintainability)?

Recommended Action: Concrete steps to address it.

Priority/Severity: (Optional, e.g., High/Medium/Low, or Critical/Major/Minor)

Code Citation Format: When pointing to code, cite using Cursor's format startLine:endLine:filepath (e.g., 42:60:src/ols_models.py) so we can jump directly to the snippet.

Example:

*   **File Path & Line Number(s)**: `src/data_processing.py:115-120`
*   **Issue/Suggestion Description**: Redundant data loading.
*   **Reasoning/Impact**: The file `some_data.csv` is loaded twice in the same function, impacting performance for large files.
*   **Recommended Action**: Load the data once into a DataFrame and reuse the DataFrame.
*   **Priority/Severity**: Medium

Please focus on actionable and constructive feedback. Thank you!