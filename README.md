# Ethereum Econometric Valuation Analysis

## ðŸ¤– AI Agent Development Workflow

This project utilizes an experimental, autonomous AI agent-driven workflow to assist with ongoing development, task management, and quality assurance. This system is orchestrated by a series of scripts (in `scripts/`) and guided by detailed instructional prompts (in `prompts/`).

Key aspects of this workflow include:
*   Automated task progression based on a structured roadmap (`prompts/roadmap.jsonl`).
*   Dynamic updates to a central agent control prompt (`prompts/starter_prompt.txt`).
*   Regular automated code quality audits (`scripts/qa_audit.py`) with results tracked in `prompts/quality_scoreboard.md`.
*   Automated code implementation, testing, PR creation, and logging for defined roadmap tasks.

While this system aims for a high degree of autonomy, human oversight is maintained, particularly for PR reviews and strategic roadmap planning. This workflow is an active component of how the `ethereum_project` evolves.

**For a detailed explanation of this AI agent workflow, its components, and operational phases, please see [`docs/AI_AGENT_WORKFLOW.md`](docs/AI_AGENT_WORKFLOW.md).**

Contributors working on the core econometric analysis can generally ignore the specifics of this agent system. However, those interested in the meta-development process or looking to modify the agent's behavior should consult the detailed documentation.

## Overview

This project conducts an econometric analysis of Ethereum (ETH) valuation, primarily exploring its relationship with network activity metrics, drawing inspiration from Metcalfe's Law. It aims to identify key drivers of ETH's value using various statistical models. The project fetches, processes, and analyzes on-chain and market data for Ethereum and benchmark assets like the NASDAQ index.

The primary execution script is `main.py`, which runs the complete end-to-end analysis pipeline. For interactive data exploration, model development, and visualization, `research.py` provides a suitable environment.

## Key Features

*   **Data Fetching**: Retrieves on-chain data (e.g., active addresses, transaction counts) and market data (prices, volumes) from various APIs.
*   **Data Processing**: Cleans, transforms, merges, and resamples raw data into daily and monthly frequencies suitable for analysis.
*   **Exploratory Data Analysis (EDA)**: Includes outlier treatment (e.g., winsorization) and stationarity testing (e.g., ADF tests) on time series data.
*   **Econometric Modeling**: Implements and evaluates several models:
    *   Ordinary Least Squares (OLS) benchmarks.
    *   Vector Error Correction Models (VECM).
    *   Autoregressive Distributed Lag (ARDL) models.
*   **Model Diagnostics**: Performs residual analysis and structural break tests.
*   **Out-of-Sample Validation**: Conducts rolling window validation to assess model robustness.
*   **Reporting**: Generates a structured JSON file (`final_results.json`) with all analysis results and prints a summary interpretation.

## Technology Stack

*   **Language**: Python
*   **Core Libraries**:
    *   `pandas` (for data manipulation)
    *   `numpy` (for numerical operations)
    *   `statsmodels` (for statistical models, e.g., OLS, ARDL, VECM diagnostics)
    *   `scikit-learn` (for utility functions like preprocessing)
    *   `matplotlib` (for data visualization)
    *   `requests` (for API communication)
    *   `pydantic` (for configuration management)
    *   `pyarrow` (for efficient Parquet file handling)
*   **Containerization**: Docker
*   **Development Tools**: (Detailed in `PROJECT_CONFIG_DETAILS.md`)
    *   `pre-commit` (for git hooks)
    *   `ruff` & `flake8` (for linting and formatting)
    *   `mypy` (for static type checking)
    *   `pytest` (for testing)

## Project Structure

```
ethereum_project/
â”œâ”€â”€ .git/                        # Git repository data
â”œâ”€â”€ .github/                     # GitHub Actions workflows (CI/CD)
â”œâ”€â”€ .venv/                       # Python virtual environment (user-created)
â”œâ”€â”€ data/                        # Raw and processed data files (e.g., .parquet)
â”œâ”€â”€ docs/                        # Project documentation (e.g., type ignore guidelines, AI_AGENT_WORKFLOW.md)
â”œâ”€â”€ htmlcov/                     # HTML code coverage reports
â”œâ”€â”€ prompts/                     # Auxiliary files for AI-assisted development
â”œâ”€â”€ scripts/                     # Utility scripts (e.g., qa_audit.py for dev checks)
â”œâ”€â”€ src/                         # Core source code
â”‚   â”œâ”€â”€ utils/                   # Utility modules (caching, API helpers, file I/O)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                # Project configuration (loads .env)
â”‚   â”œâ”€â”€ data_fetching.py         # Data retrieval logic
â”‚   â”œâ”€â”€ data_processing.py       # Data cleaning, transformation, feature engineering
â”‚   â”œâ”€â”€ diagnostics.py           # Model diagnostic tests
â”‚   â”œâ”€â”€ eda.py                   # Exploratory Data Analysis functions
â”‚   â”œâ”€â”€ main.py                  # Main pipeline script (entry point for analysis)
â”‚   â”œâ”€â”€ ols_models.py            # OLS regression models
â”‚   â”œâ”€â”€ reporting.py             # Results summarization and output generation
â”‚   â”œâ”€â”€ ts_models.py             # Time series models (VECM, ARDL)
â”‚   â””â”€â”€ validation.py            # Out-of-sample validation logic
â”œâ”€â”€ tests/                       # Automated tests
â”œâ”€â”€ .dockerignore                # Specifies files to exclude from Docker builds
â”œâ”€â”€ .gitignore                   # Specifies intentionally untracked files for Git
â”œâ”€â”€ .pre-commit-config.yaml      # Configuration for pre-commit hooks
â”œâ”€â”€ .python-version              # Specifies the project's Python version (e.g., for pyenv)
â”œâ”€â”€ Dockerfile                   # Defines the Docker image for the project
â”œâ”€â”€ LICENSE                      # Project license information
â”œâ”€â”€ main.py                      # Main script to run the full analysis pipeline (symlink or copy of src/main.py)
â”œâ”€â”€ mypy.ini                     # Configuration for mypy static type checker
â”œâ”€â”€ pip.conf                     # pip configuration (e.g., extra index URLs)
â”œâ”€â”€ PROJECT_CONFIG_DETAILS.md    # Detailed content of config files for reference
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ research.py                  # Script for interactive research and plotting
â”œâ”€â”€ requirements-dev.txt         # Dependencies for development
â””â”€â”€ requirements-lock.txt        # Pinned versions of all dependencies
```
*Note on `prompts/` and `scripts/` directories: These contain auxiliary files primarily used for the AI Agent Development Workflow. They are not required for running the core econometric analysis pipeline.*

## Modules (`src/` directory)

*   `config.py`: Manages project settings and API keys, primarily by loading them from an `.env` file.
*   `data_fetching.py`: Handles fetching raw data from external APIs (e.g., CoinMetrics, Yahoo Finance), including caching mechanisms.
*   `data_processing.py`: Cleans, transforms, merges, and resamples raw data into analysis-ready `daily_clean.parquet` and `monthly_clean.parquet` datasets.
*   `diagnostics.py`: Implements model diagnostic tests, such as residual analysis and structural break tests.
*   `eda.py`: Provides functions for exploratory data analysis, including data winsorization and stationarity tests (e.g., ADF).
*   `main.py`: The main entry point script for executing the full end-to-end analysis pipeline.
*   `ols_models.py`: Implements Ordinary Least Squares (OLS) benchmark models.
*   `reporting.py`: Generates summaries of analysis results, formats them into a structured dictionary, and handles JSON serialization.
*   `ts_models.py`: Implements time series models like Vector Error Correction Models (VECM) and Autoregressive Distributed Lag (ARDL) models.
*   `validation.py`: Handles out-of-sample model validation, particularly rolling window validation.
*   `utils/`: A sub-package containing various utility modules:
    *   `cache.py`: Caching utilities.
    *   `api_helpers.py`: Helper functions for interacting with external APIs.
    *   `file_io.py`: Utilities for file input/output.

## Setup and Installation

1.  **Clone the Repository:**
    ```bash
    git clone <your-repository-url>
    cd ethereum_project
    ```

2.  **Ensure Correct Python Version:**
    *   The project is configured to use **Python 3.11** for local development, as specified in the `.python-version` file.
    *   The `Dockerfile` uses **Python 3.12-slim** for containerized execution. CI tests cover Python 3.10-3.12.
    *   Ensure you have Python 3.11 accessible for local work.

3.  **Create and Activate Virtual Environment (using Python 3.11):**
    ```bash
    python3.11 -m venv .venv
    source .venv/bin/activate # On macOS/Linux
    # .\.venv\Scripts\activate # On Windows PowerShell
    ```
    Ensure `.venv/` is in your `.gitignore`.

4.  **Install Dependencies:**
    Install the exact dependencies from the lock file:
    ```bash
    pip install --upgrade pip
    pip install -r requirements-lock.txt
    ```

5.  **Developer Dependencies (Optional):**
    For development (running linters, type checkers, tests locally):
    ```bash
    pip install -r requirements-dev.txt
    ```
    It's recommended to set up `pre-commit` hooks:
    ```bash
    pre-commit install
    ```

6.  **Environment Variables (`.env` file):**
    Create a `.env` file in the project root for API keys.
    Example `.env` content:
    ```dotenv
    RAPIDAPI_KEY=your_rapidapi_key_here
    CM_API_KEY=your_coinmetrics_key_here # Optional
    ETHERSCAN_API_KEY=your_etherscan_key_here # Optional (currently not used by the core pipeline)
    ```
    Ensure `.env` is listed in your `.gitignore` file.

## Usage

### Full Pipeline Execution

1.  Activate your virtual environment.
2.  Ensure required environment variables (at least `RAPIDAPI_KEY`) are set.
3.  Run `main.py` from the project root:
    ```bash
    python main.py
    ```
    This executes the entire pipeline and outputs results to `final_results.json` (which is gitignored) and a console summary.

### Interactive Research

1.  Open the project in an IDE supporting interactive Python.
2.  Ensure the IDE's Python interpreter is set to the project's virtual environment.
3.  Open `research.py` for interactive data exploration and model development.

## Testing

Tests are run using `pytest`. Ensure development dependencies are installed.
```bash
pytest # Run all tests
pytest --cov=src --cov-report=xml # Run tests with coverage
```

## Running with Docker

1.  **Build the Docker Image:**
    ```bash
    docker build -t ethereum_project .
    ```
2.  **Run the Main Application in the Container:**
    ```bash
    # Using the dummy RAPIDAPI_KEY set in the Dockerfile
    docker run --rm ethereum_project
    # To use a real API key:
    # docker run --rm -e RAPIDAPI_KEY="your_actual_rapidapi_key" ethereum_project
    ```

## Configuration File Details

For detailed contents of project configuration files (e.g., `.gitignore`, `.pre-commit-config.yaml`, `mypy.ini`, GitHub Actions workflows), please refer to **`PROJECT_CONFIG_DETAILS.md`**.

## Key Dependencies (from `requirements-lock.txt`)

The project relies on several key libraries, with exact versions pinned in `requirements-lock.txt` for reproducibility. Core runtime dependencies include:

*   `pandas==2.2.2`
*   `numpy==1.26.4`
*   `statsmodels==0.14.2`
*   `scikit-learn==1.5.0`
*   `matplotlib==3.8.4`
*   `requests==2.32.3`
*   `pydantic==1.10.14`
*   `pyarrow==15.0.2`

Development and testing tools such as `pytest`, `ruff`, `mypy`, and `pre-commit` are also pinned. Please refer to `requirements-lock.txt` for the complete list of all direct and transitive dependencies.

## Notes on Python Versions & Dependencies

*   **Local Development:** Python 3.11 (specified in `.python-version`).
*   **Docker Environment:** Python 3.12 (specified in `Dockerfile`).
*   **CI Testing:** Python 3.10, 3.11, 3.12.
*   **Locked Dependencies (`requirements-lock.txt`):** Generated using `pip-compile` with Python 3.11. These are the exact versions for reproducible runs.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## Collaboration & CI (For Human Contributors)

*   **Branching**: Work on feature branches, submit Pull Requests to `main`.
*   **CI**: GitHub Actions run linters, type checkers, and tests (see `.github/workflows/`).
*   **Pre-commit**: Use `pre-commit run --all-files` locally before pushing.
