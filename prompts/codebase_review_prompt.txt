########################################################################################
# Codebase Review Prompt — **ethereum_project**
########################################################################################

# Phase 1: Codebase Review Prompt

## Overview  
**Goal:** Perform a comprehensive static review of the repository at its current HEAD commit.  
**Scope:** All source code, configuration, and documentation in the repo. Focus on areas highlighted by the previous audit while also covering general best practices.  
**Deterministic Output:** Pin the full commit SHA under review and base all analysis on this exact revision. Avoid using any data that could change between runs.  
**Constraints:** Limit the exploration to ≤200 lines of file content read and ≤10 external calls to ensure efficiency.  

## Quality Axes and Checks  
Evaluate the codebase across the following quality axes. For each axis, run the specified checks and gather findings.  

1. **Documentation & Metadata**  
   - Verify that the README (and any docs) accurately describe setup, usage, and dependencies. Ensure that instructions (e.g., installation steps or examples) align with the actual code and requirements. *(For example, if the README lists a library or command, confirm it exists and is correct in the repository.)*  
   - Cross-check declared dependencies and versions in the README against the actual `requirements.txt` / environment files. **Flag** any mismatches or drift (e.g., a package version in README that differs from the pinned requirement).  
   - Inspect the LICENSE file for completeness. **Assert** that it has correct project metadata (year, author, etc.) with no placeholder or template text. If placeholders (like “\[year\]” or boilerplate names) are present instead of real data, **flag** this as an issue.  
   - **Evaluate** the overall documentation quality: Are usage examples up-to-date and working? Is there any missing information (e.g., contribution guidelines, changelog)? Consider performing a lightweight “hello-world” walkthrough using the docs (mentally simulate following the README step-by-step) to see if anything is confusing or incorrect.  
   - **Score Criterion:** High score if documentation is clear, correct, and complete (including a proper LICENSE). Deduct points for stale instructions, missing info, or metadata issues.  

2. **Code Quality & Maintainability**  
   - **Examine** code style consistency and linting: If a linter or formatter is configured (e.g. Ruff, Black), ensure its configuration is consistent. **Check** specifically for the Ruff version discrepancy noted in the audit: compare the version pinned in `pyproject.toml` or `requirements.txt` against the version referenced in any config (like `.pre-commit-config.yaml`). **Flag** any version mismatches or outdated pins.  
   - Scan for common code smells or maintainability issues: overly complex functions, duplicated code, or obvious bugs. **Identify** any “orphan” modules or files (e.g. modules like `modeling.py` that are never imported or used). Such dead code should be highlighted as cleanup targets.  
   - **Assess** project structure and clarity: Are modules and functions well-organized? Is naming self-explanatory? Check if the code adheres to standard conventions and if the repository structure is logical (for example, no miscellaneous scripts floating without purpose).  
   - Optionally, **visualize** module relationships: If feasible, generate a simple module dependency graph (e.g., using PlantUML syntax) to illustrate how internal modules interact. This can reveal unusual couplings or unused components. *(This step is optional and for insight; skip if time is limited or not applicable.)*  
   - **Score Criterion:** High score if code is clean, well-structured, and free of obvious issues, and all linting/tool configurations are in sync. Deduct for inconsistent styles, unused code, or maintainability red flags.  

3. **Security & Dependency Management**  
   - **Scan** the repository for secrets or sensitive information. This includes looking for API keys, credentials, or private keys committed in the code. Use both automated pattern searches (e.g., `"AKIA"` for AWS keys, `"BEGIN RSA PRIVATE KEY"`) and manual inspection of config files. **Flag** any secret or key exposure **CRITICAL** if found (these should be removed and rotated).  
   - **Inspect** dependency management: Review `requirements.txt`, `pyproject.toml`, or equivalent for third-party packages. Are versions pinned (fixed) to ensure reproducible builds? **Flag** unpinned dependencies (they introduce non-determinism). Check for extremely outdated versions or known vulnerable libraries (if known offline). If a vulnerability scanning tool is available, run it and include any findings.  
   - Ensure consistency across configurations: e.g., the Python version in Dockerfile vs. CI vs. README, or library versions referenced in multiple places (as seen with Ruff). All such references should align.  
   - Generate a brief **SBOM (Software Bill of Materials)**: list the key external libraries and their versions in use. You can extract this from the requirements or lock file. This helps identify what software the project relies on.  
   - **Score Criterion:** High score if no sensitive data is in repo, dependencies are well-managed (pinned, updated, no known major vulns), and config is consistent. Deduct for any secret exposures, outdated or inconsistent dependencies, or security policy gaps.  

4. **Testing & CI**  
   - **Check** for the presence and quality of tests. Look for a `tests/` directory or test files. Are there adequate unit/integration tests covering core functionality? If possible, estimate test coverage or at least whether critical paths are tested.  
   - Review continuous integration (CI) setup: is there a GitHub Actions workflow or similar? **Verify** that linting and tests are run in CI. Note if the CI is passing or if there are failing badges.  
   - **Assess** how easy it is to run tests and the test suite results. If tests exist, can they be run with a single command (e.g., `pytest`)? Are there instructions in the README for running tests or contributing?  
   - Consider whether the testing strategy covers security (e.g., misuse cases) and if the CI includes checks for code quality (lint, format) and security (SAST or dependency audit).  
   - **Score Criterion:** High score if the project has thorough tests, a robust CI pipeline, and easy developer experience for running tests. Deduct for lack of tests, missing CI, or fragile testing processes.  

5. **DevOps & Infrastructure**  
   - **Inspect** the Dockerfile and/or deployment scripts. Ensure the Docker image is lean and production-ready, not cluttered with development-only tools. **Check** if a multi-stage build is used to reduce image size (this was noted as a potential issue). If the Dockerfile installs test or dev dependencies or includes large tools unnecessary at runtime, **flag** it.  
   - Evaluate environment reproducibility: Are there lock files (like `requirements.txt` with exact pins, or `poetry.lock`/`Pipfile.lock` if relevant)? If this is a Python project, can a developer reliably reproduce the environment? **Flag** the absence of lock files or inconsistent environment configuration.  
   - **Verify** the `.gitignore` is properly configured. Cross-check it against the repo contents for any large or generated files that should have been ignored. For example, identify any huge files or build outputs in the repository (notebooks outputs, datasets, etc.). If `.gitignore` intends to exclude them but they are present, that's a problem. List the top largest files in the repo as evidence if they seem out of place, and **flag** accordingly.  
   - **Assess** the deployment and build process: Is documentation provided for building or deploying the project (Docker usage, etc.)? Does the project specify consistent tooling (particular Python version, etc.) to avoid “works on my machine” issues?  
   - **Score Criterion:** High score if the project is easy to build and deploy, with a minimal and correct Docker image, no unwanted artifacts in VCS, and reproducible env. Deduct for heavy or dev-laden Docker images, leaked build artifacts, or unclear build steps.  

## Scoring & Reporting Guidelines  
After performing the checks above, assign a 0-100 score for each quality axis: higher means better quality/compliance. Use **professional judgment** based on findings to score fairly. Keep the scoring consistent with previous evaluations (if any) to allow trend tracking. General guidance: 
- **90-100 (Excellent):** Virtually no issues, or only trivial improvements remain. (Must provide concrete evidence or examples to justify such a high score.)  
- **70-89 (Good):** Good overall, with some minor issues or improvement opportunities.  
- **50-69 (Fair):** Noticeable issues that should be addressed, though not critical.  
- **0-49 (Poor):** Serious problems in this area. (Must provide evidence for why the score is so low and outline key failings.)  

For **any score ≥ 90 or ≤ 50**, include a specific citation of evidence: e.g., file names and line numbers or excerpt that illustrate why the score is very high or very low. This could be a code snippet showing excellence or a problematic fragment that justifies the concern.  

Next to each score, provide a **confidence rating** (High, Medium, Low) reflecting how certain you are about the assessment. Use High if the area was thoroughly checked with clear evidence, Medium if there are some uncertainties or gaps in info, and Low if the assessment had to rely on inference or limited data.  

## Output Format  
Finally, update the quality scoreboard files with the new results:  

- **Markdown Output (quality_scoreboard.md):** Begin with a heading or note identifying the commit (e.g., “**Quality Scoreboard for commit `<SHA>`**”). For each quality axis, list the axis name, the score (0-100), and the confidence. Provide a brief rationale in full sentences, including key findings and any flagged issues or highlights. Use bullet points under each axis if needed to list multiple findings or recommendations. Ensure that any critical issues are clearly marked (for example, prefix with "**CRITICAL:**" in bold). If applicable, suggest quick wins for improvement (simple fixes) in *italic* to distinguish them. For example, "*Quick win: Remove the large `data.json` file from version control and add it to .gitignore.*"  
  - If a previous scoreboard exists (e.g., from an earlier run/commit), include a short comparison for trend analysis. For instance: "*Documentation score increased by +5 compared to previous run, due to the updated README.*" Note any significant improvements or regressions for each axis.  
  - Strive to keep the markdown readable and concise, using subheadings or bold labels for each axis as needed for clarity. Wrap lines at ~100 characters for readability.  

- **JSON Output (quality_scoreboard.json):** Provide a machine-readable summary of the scores and confidence. The JSON should include the commit SHA and a structured breakdown of each axis. For example:  
  ```json
  {
    "commit": "<full 40-char SHA>",
    "axes": [
      { "name": "Documentation & Metadata", "score": 75, "confidence": "High" },
      { "name": "Code Quality & Maintainability", "score": 85, "confidence": "Medium" },
      ... 
    ]
  }
````

Ensure the JSON is valid and matches the markdown. The axis names in JSON should exactly match those in the markdown. Include any other relevant fields if needed for downstream use (such as timestamps or comparisons).

**Self-Check:** Before finalizing, verify that all listed checks have been performed and all findings are accounted for. Reflect on whether any important aspect of the repository was missed. If something important is uncovered late, incorporate it into the respective axis assessment and adjust the score if necessary. Only finalize the scores once confident that the review is thorough.